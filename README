README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. Information on how to do this
	is in the flex manual, which is part of your reader.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turnin your work type:

	% make submit-clean

	After that,  collect the files cool.flex, test.cl,
	README, and test.output into a .tar.gz or .zip file and submit
	in our moodle page. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------
The main goal of this assignment is to write a lexical analyzer for COOL. For that matter, we use flex, which is a tool for generating scanners. The scanner reads an input stream and produces a stream of tokens. It is implemented in the file cool.flex and is invoked by the function yylex(), defined in the file cool-lexer.cc.

It is possible to divide the task of writing a lexical analyzer into two parts: the specification of the lexical structure of the language and the implementation of the scanner. The first part is done in cool.flex, while the second part is done by flex. The specification of the lexical structure of the language is done by writing regular expressions that describe the tokens of the language. The implementation of the scanner is done by writing actions that are executed when a token is recognized.

In cool.flex, we tried as much as possible to make the code clean and easy to understand. We use definitions to make the task of working with regular expressions easier and make the code more readable. They are segregated into single-character, multiple-character digits and ID tokens.

After the definitions, we have the rules, in which we establish the regular expressions and the actions to be executed when a token is recognized. The most important aspect here is to make sure that the regular expressions and definitions are ordered correctly. This impacts directly on the behavior of the scanner, since some expressions can be recognized by multiple patterns. In cool.flex, we ordered the rules in the following way:
 - More specif patterns always come first e.g. the keyword 'class' could be identified as an ID, but it is more specific than the ID pattern, so the regex that recognizes 'class' and returns the 'CLASS' token comes first.

One key aspect for the correct behavior of the scanner and readability is defining start conditions scopes. This is done by using the %x directive. In cool.flex, we use it to change the lexical analyzer's context when we find a certain pattern. We will take the STR scope as an example. Every time double quotes are found, the context of the lexical analyzer is changed to STR: BEGIN(STR) is called and the scanner will start recognizing tokens using the rules defined in '<STR> {}'
This means that the rules defined in the STR scope will be used to recognize tokens until we change the context again -  the next time unescaped " are found - when BEGIN(INITIAL) is called to retake the default condition. This way, we can recognize strings, comments and treat some errors correctly.

Lexical Erros
It is the scanner's responsability to deal with lexical errors. In cool.flex, we treat the following errors:
	- Unterminated string - when the scanner finds a string that is not terminated by a double quote
	- EOF in comment - when the scanner finds a comment that is not terminated by a *)
	- Illegal character - when the scanner finds a character that is not recognized by any of the rules, it return the character.
	- NULL character - when the scanner finds a null character in a string
	- Long string - when the scanner finds a string that is longer than 1024 characters
	in the previous two errors, the scanner switches the context to TREAT_STR_ERROR. This start condition is used to discard the rest of the string and resume the normal behavior of the scanner safely.
In all cases, the lexical analyzer returns the token ERROR, which is used by the parser to treat the error. The scanner also manages the current line number of execution, which is used by the parser to print the error message.

At last, we specify the user code. This part represents auxiliary functions that are executed by the actions of the rules involving strings. We have the following functions:
 - void read_char(char ch) - reads a character, appends it to the string buffer and checks if the string is too long. If it is, it calls long_str_error()
 - int long_str_error() - adds the apropriate text to cool_yylval.error_msg, changes the scope to TREAT_STR_ERROR and returns the ERROR token.